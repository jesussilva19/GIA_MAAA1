{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfbc34d4",
   "metadata": {},
   "source": [
    "# Creando wrappers de modelos para integrarlos con *Scikit-Learn*\n",
    "\n",
    "\n",
    "En este notebook, se mostrará cómo crear un wrapper para un modelo de `Flux` en Julia y usarlo en un `Pipeline` junto con `GridSearchCV` para encontrar los mejores hiperparámetros. Este procedimiento es idéntico para integrar cualquier tipo de modelo. Simplemente es necesario tener en cuenta dos consideraciones. \n",
    "\n",
    "La primera es que el modelo debe de implementar el interface de *ScikitLearnBase*, o al menos como se verá más adelante aquellas funciones necesarias para poder ejecutarse.\n",
    "\n",
    "La segunda de las consideraciones es que, dado que en Julia Sciki-Learn es un wrapper a su vez del paquete de Python, una vez que se llame a dicho paquete, no se puede volver a Julia y todos los modelos deberan de devolver los resultados a Julia antes de proseguir. Por poner un ejemplo claro, si intetásemos usar GridSearchCV con de Python, el modelo en FLux no podría integrarse ya que los resultados intermedios no son accesibles desde Python a la matriz de Julia. En cambio, si se usa como se verá más adelante el paquete GrisSearchCV implementado en Julia, si que se obtienen los resultados de los paquetes Python y por tanto funcionará correctamente.\n",
    "\n",
    "\n",
    "## 1. Instalación de Paquetes\n",
    "\n",
    "El primero de los puntos es asegurarnos de que la librería `Flux`, que es una biblioteca de aprendizaje automático para Julia, está instalada. \n",
    "\n",
    "``` julia\n",
    "using Pkg\n",
    "Pkg.add([\"Flux\", \"ScikitLearn\", \"ScikitLearnBase\", \"Statistics\"])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a51ca4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ScikitLearnBase: BaseClassifier, fit!, predict, score, @declare_hyperparameters, is_classifier # Se explica más adelante\n",
    "using Flux\n",
    "using Flux.Losses\n",
    "using Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca9f7d4",
   "metadata": {},
   "source": [
    "## 2. Creación de una estructura contenedora\n",
    "\n",
    "Para la creación del *wrapper* lo primero en Python sería crear una clase que corrobore un cierto *interface*, es decir un conjunto de firmas de métodos que nos permitan hacer llamadas de manera uniforme y reconocible para la librería ScikitLearn.\n",
    "\n",
    "En Julia a diferencia de Python u otros lenguajes Orientados a Objetos, no existen las clases como tal. Pero no es un problema ya que se puede simular con una esructura mutable. Estas estructuras permiten cambiar los valores y almacenar llamadas a diferentes métodos, de manera similar a una clase, además  servira de tipo para la sobrecarga de los métodos.\n",
    "\n",
    "El primer paso es la importación del interface, en este caso, `ScikitLearnBase` que  aportará todas las firmas de métodos que se van a necesitar. En este caso véase un ejemplo con las redes de classificación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df1f87d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct ClassANN <: BaseClassifier\n",
    "    # Hiperparámetros del modelo (no aprendidos de los datos)\n",
    "    topology::AbstractVector{Int}\n",
    "    transferFunctions::AbstractVector{Function}\n",
    "    maxEpochs::Int\n",
    "    minLoss::Real\n",
    "    learningRate::Real\n",
    "\n",
    "    # Parámetros aprendidos (modelo de Flux y optimizador)\n",
    "    model::Chain\n",
    "    opt::ADAM\n",
    "\n",
    "    # Constructor que acepta los hiperparámetros como argumentos con nombre\n",
    "    ClassANN(; topology=[1], transferFunctions=fill(σ, 1), maxEpochs=1000, minLoss=0.0, learningRate=0.01) =\n",
    "        new(topology, transferFunctions, maxEpochs, minLoss, learningRate, Chain(), ADAM(learningRate))\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adfeefc",
   "metadata": {},
   "source": [
    "En dicho código se observa una primera parte de parámetros que serán los necesarios para el constructor, y que será responsabilidad del usuario definir. A continuación se encuentran parámetros utilitarios que serán extraidos de los datos pero que no se expondrán al exterior y finalmente la definición del constructor de esta \"pseudo-clase\". Esta última cuenta con los valores por defecto del conjunto.\n",
    "\n",
    "Para finalizar la definición se hace necesario definir dos cosas adiciones, la primera un método que declara el tipo de problema que es capaz de resolver, en el caso del ejemplo de clasificación. Para ello es necesaria la siguiente llamada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddbb2114",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is_classifier (generic function with 3 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Indicar que ClassANN es un clasificador\n",
    "is_classifier(::ClassANN) = true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3759b87",
   "metadata": {},
   "source": [
    "El segundo de los elementos necesarios es declarar aquellos parámetros que es necesario aportar por parte del usuario para construir una instancia de este tipo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c83000b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@declare_hyperparameters(ClassANN, [:topology, :transferFunctions, :maxEpochs, :minLoss, :learningRate])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9c70a5",
   "metadata": {},
   "source": [
    "## 3. Fuciones auxiliares\n",
    "A continuación se definirían todas las funciones auxiliares necesarias para la ejecución del modelo, esto se puede hacer dentro del las propias llamadas a `fit!`, `predict` y `score`. Otra opción es utilizar las ya implementadas en ocasiones anteriores como pueden ser las implementadas en la asignaturad e Fundamentos de Aprendizaje Automático para ello se puede emplear la siguiente llamada\n",
    "\n",
    "``` julia\n",
    "    include(\"MipaqueteFunciones.jl\")\n",
    "\n",
    "```\n",
    "Sin embargo por una cuestión de autocontención a continuación se pone como ejemplo un código de las funciones necesarias para este ejemplo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83052b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funcion para realizar la codificacion, recibe el vector de caracteristicas (uno por patron), y las clases\n",
    "function oneHotEncoding(feature::AbstractArray{<:Any,1}, classes::AbstractArray{<:Any,1})\n",
    "    # Primero se comprueba que todos los elementos del vector esten en el vector de clases (linea adaptada del final de la practica 4)\n",
    "    @assert(all([in(value, classes) for value in feature]));\n",
    "    numClasses = length(classes);\n",
    "    @assert(numClasses>1)\n",
    "    if (numClasses==2)\n",
    "        # Si solo hay dos clases, se devuelve una matriz con una columna\n",
    "        oneHot = reshape(feature.==classes[1], :, 1);\n",
    "    else\n",
    "        # Si hay mas de dos clases se devuelve una matriz con una columna por clase\n",
    "        # Cualquiera de estos dos tipos (Array{Bool,2} o BitArray{2}) vale perfectamente\n",
    "        # oneHot = Array{Bool,2}(undef, length(targets), numClasses);\n",
    "        #oneHot =  BitArray{2}(undef, length(feature), numClasses);\n",
    "        oneHot = classes'.== features\n",
    "    end;\n",
    "    return oneHot;\n",
    "end;\n",
    "\n",
    "# Esta funcion es similar a la anterior, pero si no es especifican las clases, se toman de la propia variable\n",
    "oneHotEncoding(feature::AbstractArray{<:Any,1}) = oneHotEncoding(feature, unique(feature));\n",
    "\n",
    "# Sobrecargamos la funcion oneHotEncoding por si acaso pasan un vector de valores booleanos\n",
    "#  En este caso, el propio vector ya está codificado, simplemente lo convertimos a una matriz columna\n",
    "oneHotEncoding(feature::AbstractArray{Bool,1}) = reshape(feature, :, 1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4317e0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "buildClassANN (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Función para la construcción de la Red Neuronal Artificial\n",
    "function buildClassANN(numInputs::Int, topology::AbstractVector{Int}, numOutputs::Int;\n",
    "            transferFunctions::AbstractVector{Function}=fill(σ, length(topology)))\n",
    "    layers = []\n",
    "    numInputsLayer = numInputs\n",
    "\n",
    "    for (i, numNeurons) in enumerate(topology)\n",
    "        push!(layers, Dense(numInputsLayer, numNeurons, transferFunctions[i]))\n",
    "        numInputsLayer = numNeurons\n",
    "    end\n",
    "\n",
    "    if numOutputs == 1\n",
    "        push!(layers, Dense(numInputsLayer, 1, σ))\n",
    "    else\n",
    "        push!(layers, Dense(numInputsLayer, numOutputs, identity))\n",
    "        push!(layers, softmax)\n",
    "    end\n",
    "\n",
    "    return Chain(layers...)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f908dec5",
   "metadata": {},
   "source": [
    "También nos podría hacer falta la función `trainANNClass`pero en este caso, por razones meramente formativas, se hará una implementación sencilla en la propia función para ejemplificarlo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70524561",
   "metadata": {},
   "source": [
    "## 4. Implementar las funciones de *Scikit-Learn*\n",
    "\n",
    "Como ya se indicó, es necesario implemtar los métodos requeridos por ScikitLearnBase, en el caso de la clasificació, `fit!`, `predict`, y `score`.\n",
    "\n",
    "En primer lugar, la función `fit!` para realizar el ajuste con el uso de Flux para el entrenamiento del modelo, en caso de tenerlo separado en una función se podría emplear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75e3951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementar el ajuste (fit!) del modelo\n",
    "function fit!(model::ClassANN, X, y)\n",
    "    numInputs = size(X, 2)\n",
    "    numOutputs = length(unique(y))\n",
    "\n",
    "    # Construir el modelo usando Flux\n",
    "    model.model = buildClassANN(numInputs, model.topology, numOutputs, transferFunctions=model.transferFunctions)\n",
    "    model.opt = ADAM(model.learningRate)\n",
    "\n",
    "    # Convertir las etiquetas a formato one-hot si es clasificación multiclase\n",
    "    if numOutputs > 1\n",
    "        y = oneHotEncoding(y, unique(y))\n",
    "    end\n",
    "\n",
    "    # Definir la función de pérdida\n",
    "    loss(x, y) = Flux.crossentropy(model.model(x), y)\n",
    "\n",
    "    # Entrenar el modelo\n",
    "    # Alternativamente\n",
    "    #(model, results) = trainANNClass(model.topology, (X', y')], model.maxEpoachs, model.minLoss, model.learningRate)\n",
    "    for epoch in 1:model.maxEpochs\n",
    "        Flux.train!(loss, Flux.params(model.model), [(X', y')], model.opt)\n",
    "        current_loss = loss(X', y')\n",
    "        println(\"Epoch: $epoch, Loss: $current_loss\")\n",
    "        if current_loss <= model.minLoss\n",
    "            break\n",
    "        end\n",
    "    end\n",
    "    return model\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e73690f",
   "metadata": {},
   "source": [
    "Se empleará tambié la función de `predict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5f0c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementar la predicción (predict) del modelo\n",
    "function predict(model::ClassANN, X)\n",
    "    if size(model.model(X'), 1) > 1\n",
    "        return Flux.onecold(model.model(X'), 1:size(model.model(X'), 1))\n",
    "    else\n",
    "        return round.(model.model(X'))\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ee0674",
   "metadata": {},
   "source": [
    "Y la función de `score`porque vamos a usar este *wrapper* dentro de un `GridSearchCV`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f00411a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función adicional para calcular el puntaje (score) del modelo\n",
    "function score(model::ClassANN, X, y)\n",
    "    predictions = predict(model, X)\n",
    "    return mean(predictions .== y)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d430ff14",
   "metadata": {},
   "source": [
    "### 4.1 Transformadores y regresión\n",
    "\n",
    "En caso de emplearse un wrapper de otro método como puede ser un PCA o alguna técnica de aprendizaje no supervisado. Sería necesrio implementar las funciones `fit!`, `transform` y `fit_transform`.\n",
    "POr su parte en el caso de implementar un modelo de regresión, sería preciso implementar `fit!`, `predict` `score`y en la mayoría de los casos `predict_proba`. Consulte el API en [Scikit Learn Model API](https://scikitlearnjl.readthedocs.io/en/latest/api/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee524dcc",
   "metadata": {},
   "source": [
    "# 5. La interacción con las librería\n",
    "\n",
    "El código anterior puede salvarse en un fichero y cargarlo con una un include o mediante un módulo que se emplee a posteriori, en dicho caso acuerdese de hacer el export de las funciones necesarias del módulo.\n",
    "\n",
    "En todo caso en el código siguiente también puede ver como llamar esas funciones y como se integran dentro de un `Pipeline` se puede cambiar para buscar la mejor combinación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa37b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fichero Test.jl\n",
    "\n",
    "using ScikitLearn\n",
    "using ScikitLearn.Pipelines: Pipeline, named_steps\n",
    "using ScikitLearn.GridSearch: GridSearchCV         #IMPORTANTE usar la implementación en Julia\n",
    "#@sk_import model_selection: GridSearchCV          # Esta es la implementación en Python y dará error al no encontrar el Wrapper\n",
    "\n",
    "@sk_import decomposition: PCA\n",
    "@sk_import datasets: load_iris\n",
    "\n",
    "# Cargar los datos\n",
    "iris = load_iris()\n",
    "X = iris[\"data\"]\n",
    "y = iris[\"target\"]\n",
    "\n",
    "# Definir la búsqueda en cuadrícula (hiperparámetros a probar)\n",
    "param_grid = Dict(\n",
    "    \"ann__maxEpochs\" => [500, 1000], \n",
    "    \"ann__learningRate\" => [0.01, 0.1]\n",
    ")\n",
    "\n",
    "#Crear una instancia de ClassANN\n",
    "# Definir los parámetros\n",
    "topology = [3, 4]\n",
    "functions = [σ, σ]\n",
    "maxEpochs = 1000\n",
    "minLoss = 0.0\n",
    "learningRate = 0.01\n",
    "\n",
    "# Crear una instancia de ClassANN\n",
    "ann = ClassANN(topology=topology, transferFunctions=functions, maxEpochs=maxEpochs, minLoss=minLoss, learningRate=learningRate)\n",
    "\n",
    "estimators = [(\"pca\",PCA()),(\"ann\",ann)]\n",
    "pipe = Pipeline(estimators)\n",
    "\n",
    "# Configurar el GridSearchCV\n",
    "grid_search = GridSearchCV(pipe, param_grid)\n",
    "\n",
    "# Ajustar el modelo usando GridSearchCV\n",
    "fit!(grid_search, X, y)\n",
    "\n",
    "# Obtener el mejor modelo y sus hiperparámetros\n",
    "println(\"Mejor modelo: \", grid_search.best_estimator_)\n",
    "println(\"Mejores hiperparámetros: \", grid_search.best_params_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.0",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
